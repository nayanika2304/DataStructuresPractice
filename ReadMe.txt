# Big O notation

--general recursive functions - O( branches^depth )
--when algoriths divide the data structures at each step(binary search tree)
  - O(log N)
--for loop inside a for - O(n*n)
--two different fors - O(n+m)
--for hash tables - worst case is O(n) and general is O(1)
-- inserting in an array - inserting N elements takes O(N) work total
-- string concatenation takes O(n^2)

############################################################################
#Data structures

# Difference between python arrays and lists

Python’s arrays are homogenous -
you can only store the same type of values (e.g., only floats) in an array.
Lists are heterogenous - you can store pretty much anything in a list -
the first element could a string, the second an integer, the third a dict, etc.
That’s the big difference.
You can also do different operations with them -
use dir() to find out what methods they support.

PS: If you know that you only need to store the same type of values
and that type is supported by Python’s array type,
use that - it is more memory-efficient.

Python arrays can be operated upon like array/3 but lists cannot

# Hash tables in Python

In Python, the Dictionary data types represent the implementation of hash tables.
The Keys in the dictionary satisfy the following requirements.

The keys of the dictionary are hashable i.e. the are generated by hashing function
which generates unique result for each unique value supplied to the hash function.
The order of data elements in a dictionary is not fixed.

So the search and insertion function of a data element becomes much faster
as the key values themselves
become the index of the array which stores the data.

# Graphs

For example computer network topology or
analysing molecular structures of chemical compounds.
They are also used in city traffic or route planning and
even in human languages and their grammar.

###################################################################################

# Algorithms

# Depth first search - Tree

Time Complexity: O(n)
Auxiliary Space : If we don’t consider size of stack for function calls
then O(1) otherwise O(n).

Therefore, the Depth First Traversals of this Tree will be:
(a) Inorder   (Left, Root, Right)

    In case of binary search trees (BST),
    Inorder traversal gives nodes in non-decreasing order.
    To get nodes of BST in non-increasing order, a variation of Inorder traversal
    where Inorder traversal s reversed can be used.

(b) Preorder  (Root, Left, Right)

    Preorder traversal is used to create a copy of the tree.
    Preorder traversal is also used to get prefix expression
    on of an expression tree.
    Please see http://en.wikipedia.org/wiki/Polish_notation
    to know why prefix expressions are useful.

(c) Postorder (Left, Right, Root)

    Postorder traversal is used to delete the tree.
    Please see the question for deletion of tree for details.
    Postorder traversal is also useful to get the postfix expression of an expression tree.

# Breadth first traversal - Tree

it goes same as BFS for graph ( layer by layer using queue)
Time complexity is O(n) as each is visted once
in worst case scenario it could be O(n^2)
Space complexity is O(n)

# Depth first traversal - Graph

Time complexity: O(V + E), where V is the number of vertices and
E is the number of edges in the graph.
Space Complexity: O(V).
Since, an extra visited array is needed of size V.

# Bread First traversal - Graph

we visit the graph layer by layer
start with root node(arbitrary node)
if adjacent nodes are visited, visited[] has true else false
append the visited nodes in the queue
while the queue has len
pop the first element from queue and check for it`s adjacent nodes
and do the above process again

Time Complexity: O(V+E) where V is number of vertices in the graph and
E is number of edges in the graph.

# Binary Search

time complexity to O(Log n)

# Merge sort

Time complexity of Merge Sort is O(nLogn) in all 3 cases (worst, average and best)
as merge sort always divides the array into two halves and
take linear time to merge two halves.

O(n) space complexity

# Quick sort
O(nLogn) time complexity
o(1) space complexity

##########################################################################

# Concepts of bit manipulation

https://www.hackerearth.com/practice/basic-programming/bit-manipulation/basics-of-bit-manipulation/tutorial/
https://codeburst.io/your-guide-to-bit-manipulation-48e7692f314a
https://www.youtube.com/watch?v=NLKQEOgBAnw
https://www.interviewcake.com/concept/java/bit-shift

https://www.youtube.com/watch?v=fDKUq38H2jk
arithmetic left and righ shift : https://www.youtube.com/watch?v=nSKT6Ph8u9Q

get the ith bit : x & (1<<i) !== 0
set the ith bit : x | ( 1 << i)
clear the ith bit : x & (~( 1 << i))

 c = c & (c - 1) will clear the least significant bit in c.

Subtract binary numbers : https://www.youtube.com/watch?v=OHf0Eg9ZhGQ
add binary numbers : https://www.youtube.com/watch?v=sJXTo3EZoxM
# Memory Stack and Heap

https://gribblelab.org/CBootCamp/7_Memory_Stack_vs_Heap.html

# Recursion

https://www.freecodecamp.org/news/how-recursion-works-explained-with-flowcharts-and-a-video-de61f40cb7f9/

# Dynamic Programming

Dynamic Programming is mainly an optimization over plain recursion.
Wherever we see a recursive solution that has repeated calls for same inputs,
we can optimize it using Dynamic Programming.

For example, if we write simple recursive solution for Fibonacci Numbers,
we get exponential time complexity and if we optimize it by storing solutions of subproblems,
time complexity reduces to linear.